{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Information\n",
    "\n",
    "The dataset contains sonar signals used to distinguish between metal cylinders (mines) and rocks. There are 111 patterns from metal cylinders and 97 from rocks, collected under various conditions and angles. Each pattern includes 60 numbers representing the energy in specific frequency bands, with higher frequencies occurring later. The labels are \"R\" for rocks and \"M\" for mines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "font = {'family' : 'Georgia',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "\n",
    "COLOR = 'gray'\n",
    "mpl.rcParams['text.color'] = COLOR\n",
    "mpl.rcParams['axes.labelcolor'] = COLOR\n",
    "mpl.rcParams['xtick.color'] = COLOR\n",
    "mpl.rcParams['ytick.color'] = COLOR\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Metrics\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.preprocessing import RobustScaler, MaxAbsScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('sonar.all-data-uci.csv')\n",
    "data.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign feature columns and target column\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(classifiers, classifier_names, scalers, scaler_names, X, y):\n",
    "    results = {}\n",
    "    for scaler, scaler_name in zip(scalers, scaler_names):\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        train_x, test_x, train_y, test_y = train_test_split(X_scaled, y, test_size=0.33)\n",
    "\n",
    "        for clf, clf_name in zip(classifiers, classifier_names):\n",
    "            clf.fit(train_x, train_y)\n",
    "            y_pred = clf.predict(test_x)\n",
    "            score = accuracy_score(test_y, y_pred)\n",
    "            # print(f\"Scaler: {scaler_name}, Classifier: {clf_name}, Accuracy: {score:.2f}\")\n",
    "\n",
    "            cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=1)\n",
    "            cv_scores = cross_val_score(clf, X_scaled, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "            results[(scaler_name, clf_name)] = cv_scores\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define scalers and classifiers\n",
    "scaler_names = ['MinMax', 'MaxAbs', 'Robust']\n",
    "scalers = [MinMaxScaler(), MaxAbsScaler(), RobustScaler()]\n",
    "\n",
    "classifier_names = [\n",
    "    \"K Nearest Neighbors\", \n",
    "    \"Linear SVC\", \n",
    "    \"RBF SVC\", \n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\", \n",
    "    \"Random Forest\", \n",
    "    \"Neural Net\",\n",
    "    \"Gaussian Naive Bayes\", \n",
    "    \"Gradient Boost\", \n",
    "    \"Logistic Regression\"\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3), \n",
    "    SVC(kernel=\"linear\", C=0.025), \n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)), \n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000), \n",
    "    GaussianNB(), \n",
    "    GradientBoostingClassifier(), \n",
    "    LogisticRegression()\n",
    "]\n",
    "\n",
    "\n",
    "# Perform cross-validation\n",
    "results = cross_validation(classifiers, classifier_names, scalers, scaler_names, X_train, y_train)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "data = []\n",
    "for (scaler_name, clf_name), scores in results.items():\n",
    "    mean_accuracy = np.mean(scores)\n",
    "    data.append((scaler_name, clf_name, mean_accuracy))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Scaler', 'Classifier', 'Accuracy'])\n",
    "df['Combination'] = df['Scaler'] + ' + ' + df['Classifier']\n",
    "\n",
    "# Filter out combinations with accuracy below 50% since they are worse than taking a random guess \n",
    "df_filtered = df[df['Accuracy'] >= 0.50]\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.barh(df_filtered['Combination'], df_filtered['Accuracy'], color='skyblue')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Classifier and Scaler Combinations vs. Accuracy')\n",
    "plt.xlim(0.60, 1.0)  # Set x-axis to start at 60%\n",
    "plt.grid(axis='x')\n",
    "plt.xticks(ticks=np.arange(0.60, 1.01, 0.05), labels=[f'{int(x * 100)}%' for x in np.arange(0.60, 1.01, 0.05)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear from the chart that Gaussian Process is the strongest classifier and for scalers MaxAbs slightly outperforms MinMax. So we will continue with MaxAbs and Gaussian Process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Standardize the feature values using Robust Scaler\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the expanded parameter grid\n",
    "param_grid = {\n",
    "    'kernel': [\n",
    "    1.0 * RBF(length_scale) for length_scale in [0.1, 1.0, 10.0]\n",
    "    ] + [\n",
    "    C(1.0, (1e-6, 1e2)) * RBF(length_scale, (1e-6, 1e6)) for length_scale in [0.01, 0.1, 1.0]\n",
    "    ],\n",
    "    'n_restarts_optimizer': [0, 1, 2],\n",
    "    'max_iter_predict': [5, 10, 15, 100],\n",
    "    'multi_class': ['one_vs_rest', 'one_vs_one'],\n",
    "    'optimizer': ['fmin_l_bfgs_b', None],\n",
    "    'n_jobs': [-1]  # Use all available cores\n",
    "}\n",
    "\n",
    "# Initialize the classifier\n",
    "gpc = GaussianProcessClassifier()\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(estimator=gpc, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Train the classifier with the best parameters\n",
    "best_gpc = grid_search.best_estimator_\n",
    "best_gpc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_best_gpc = best_gpc.predict(X_test_scaled)\n",
    "accuracy_best_gpc = accuracy_score(y_test, y_pred_best_gpc)\n",
    "\n",
    "# Generate the classification report\n",
    "report_dict = classification_report(y_test, y_pred_best_gpc, output_dict=True)\n",
    "\n",
    "# Convert the classification report to a DataFrame\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "print(\"Best Gaussian Process Classifier\")\n",
    "print(f\"Accuracy: {accuracy_best_gpc:.2f}\")\n",
    "\n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.796190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.066515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.595238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.738095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Accuracy\n",
       "count  100.000000\n",
       "mean     0.796190\n",
       "std      0.066515\n",
       "min      0.595238\n",
       "25%      0.738095\n",
       "50%      0.809524\n",
       "75%      0.833333\n",
       "max      0.928571"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('sonar.all-data-uci.csv')\n",
    "\n",
    "# Assign feature columns and target column\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "def experiment():\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    scaler = MaxAbsScaler()\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    # Define the kernel with appropriate bounds\n",
    "    base_kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-5, 1e5))\n",
    "\n",
    "    # Set up the best parameters for the classifier\n",
    "    best_params = {\n",
    "        'kernel': base_kernel,\n",
    "        'max_iter_predict': 10,\n",
    "        'multi_class': 'one_vs_rest',\n",
    "        'n_jobs': -1,\n",
    "        'n_restarts_optimizer': 0,\n",
    "        'optimizer': 'fmin_l_bfgs_b'\n",
    "    }\n",
    "\n",
    "    # Initialize the Gaussian Process Classifier with the best parameters\n",
    "    best_gpc = GaussianProcessClassifier(**best_params)\n",
    "\n",
    "    # Train the classifier with the training data\n",
    "    best_gpc.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    y_pred_best_gpc = best_gpc.predict(X_test_scaled)\n",
    "    accuracy_best_gpc = accuracy_score(y_test, y_pred_best_gpc)\n",
    "\n",
    "    # Generate the classification report\n",
    "    # report_dict = classification_report(y_test, y_pred_best_gpc, output_dict=True)\n",
    "\n",
    "    # Convert the classification report to a DataFrame\n",
    "    # report_df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "    # Print the results\n",
    "    # print(\"Best Gaussian Process Classifier\")\n",
    "    # print(f\"Accuracy: {accuracy_best_gpc:.2f}\")\n",
    "    # print(\"Classification Report:\")\n",
    "    # print(report_df)\n",
    "    return accuracy_best_gpc\n",
    "\n",
    "# Run the experiment multiple times and save the accuracies\n",
    "num_iterations = 100\n",
    "accuracies = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    accuracy = experiment()\n",
    "    accuracies.append(accuracy)\n",
    "    # print(f\"Iteration {i+1}/{num_iterations}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Save the accuracies to a DataFrame\n",
    "accuracies_df = pd.DataFrame(accuracies, columns=['Accuracy'])\n",
    "\n",
    "accuracies_df.describe()\n",
    "\n",
    "# Calculate the y-axis limits dynamically\n",
    "# y_min = max(0, min(accuracies) - 0.05)  # Adding a little padding\n",
    "# y_max = min(1, max(accuracies) + 0.05)  # Adding a little padding\n",
    "\n",
    "# # Plot the accuracies as a box plot with individual points\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.boxplot(accuracies, vert=True, patch_artist=True)\n",
    "# plt.scatter([1] * len(accuracies), accuracies, color='red', zorder=2)\n",
    "# plt.title('Box Plot of Accuracies over Multiple Iterations')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xticks([1], ['Accuracies'])\n",
    "# plt.ylim(y_min, y_max)  # Dynamically setting y-axis limits\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- The classifier performs exceptionally well with high precision and recall for both classes.\n",
    "- The perfect precision for class 'M' and perfect recall for class 'R' indicate very few false positives for class 'M' and no false negatives for class 'R'.\n",
    "- The high overall accuracy 95.24% and balanced macro and weighted averages suggest that the model is reliable and robust across different classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
